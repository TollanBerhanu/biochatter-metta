Model name,Size,Version,Quantisation,Mean,SD
gpt-4,,,,0.9,0.12
gpt-3.5-turbo,,,,0.88,0.16
llama-2-chat,70,ggufv2,Q4_K_M,0.66,0.41
llama-2-chat,70,ggufv2,Q5_K_M,0.57,0.37
llama-2-chat,7,ggufv2,Q3_K_M,0.52,0.34
llama-2-chat,7,ggufv2,Q5_0,0.49,0.32
llama-2-chat,7,ggufv2,Q5_K_M,0.47,0.38
llama-2-chat,13,ggufv2,Q5_0,0.46,0.53
llama-2-chat,7,ggufv2,Q4_0,0.45,0.39
llama-2-chat,7,ggufv2,Q4_K_S,0.44,0.37
llama-2-chat,13,ggufv2,Q4_1,0.43,0.48
llama-2-chat,7,ggufv2,Q4_1,0.42,0.35
llama-2-chat,13,ggufv2,Q4_0,0.41,0.4
llama-2-chat,7,ggufv2,Q4_K_M,0.41,0.35
llama-2-chat,7,ggufv2,Q8_0,0.4,0.34
llama-2-chat,13,ggufv2,Q5_K_M,0.4,0.43
llama-2-chat,70,ggufv2,Q2_K,0.38,0.42
llama-2-chat,13,ggufv2,Q4_K_S,0.38,0.39
llama-2-chat,13,ggufv2,Q8_0,0.38,0.4
llama-2-chat,13,ggufv2,Q4_K_M,0.38,0.4
llama-2-chat,13,ggufv2,Q6_K,0.38,0.4
llama-2-chat,7,ggufv2,Q6_K,0.36,0.34
mixtral-instruct-v0.1,"46,7",ggufv2,Q2_K,0.35,0.39
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_0,0.35,0.38
mixtral-instruct-v0.1,"46,7",ggufv2,Q4_K_M,0.35,0.37
llama-2-chat,70,ggufv2,Q3_K_M,0.35,0.37
llama-2-chat,13,ggufv2,Q3_K_M,0.34,0.35
mixtral-instruct-v0.1,"46,7",ggufv2,Q5_0,0.33,0.28
llama-2-chat,7,ggufv2,Q2_K,0.33,0.3
mixtral-instruct-v0.1,"46,7",ggufv2,Q6_K,0.3,0.4
llama-2-chat,13,ggufv2,Q2_K,0.3,0.4
mixtral-instruct-v0.1,"46,7",ggufv2,Q3_K_M,0.28,0.3
mixtral-instruct-v0.1,"46,7",ggufv2,Q8_0,0.28,0.34
